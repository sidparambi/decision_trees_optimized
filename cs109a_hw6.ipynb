{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"cs109a_hw6.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science\n",
    "\n",
    "## Homework 6: Trees, Bagging, Random Forests, and Boosting\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2022**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai\n",
    "\n",
    "<hr style=\"height:2pt\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 {\n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #63ACBE;\n",
       "    color: black;\n",
       "}\n",
       "h2 {\n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #f8b4ab;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #ffd0d0;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #63ACBE;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc {\n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A;\n",
       "\tborder-left: 5px solid #601A4A;\n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 {\n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left;\n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE;\n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left;\n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD;\n",
       "    color: black;\n",
       "}\n",
       "span.emph {\n",
       "\tcolor: #601A4A;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2021-CS109A/master/\"\n",
    "    \"themes/static/css/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# pandas tricks for better display\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "- **THIS IS AN INDIVIDUAL ASSIGNMENT. Collaboration on this homework IS NOT PERMITTED.**\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "- Plots should be legible and interpretable without having to refer to the code that generated them, including labels for the $x$- and $y$-axes as well as a descriptive title and/or legend when appropriate.\n",
    "- When asked to interpret a visualization, do not simply describe it (e.g., \"the curve has a steep slope up\"), but instead explain what you think the plot *means*.\n",
    "- The use of 'hard-coded' values to try and pass tests rather than solving problems programmatically will not receive credit.\n",
    "- The use of *extremely* inefficient or error-prone code (e.g., copy-pasting nearly identical commands rather than looping) may result in only partial credit.\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. Please get course staff approval before importing any additional 3rd party libraries.\n",
    "- Enable scrolling output on cells with very long output.\n",
    "- Feel free to add additional code or markdown cells as needed.\n",
    "- Ensure your code runs top to bottom without error and passes all tests by restarting the kernel and running all cells. This is how the notebook will be evaluated (note that this can take a few minutes). \n",
    "- **You should do a \"Restart Kernel and Run All Cells\" before submitting to ensure (1) your notebook actually runs and (2) all output is visible**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook contents\n",
    "\n",
    "- [**Overview and data description**](#intro)\n",
    "\n",
    "\n",
    "- [**Question 1: Decision Tree [21 pts]**](#part1)\n",
    "\n",
    "\n",
    "- [**Question 2: Bagging [20 pts]**](#part2) \n",
    "\n",
    "\n",
    "- [**Question 3: Random Forests [14 pts]**](#part3) \n",
    "\n",
    "\n",
    "- [**Question 4: Boosting [30 pts]**](#part4) \n",
    "\n",
    "\n",
    "- [**Question 5: Understanding [15 pts]**](#part5) \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"intro\"></a>\n",
    "\n",
    "## Overview and data description\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "\n",
    "### Higgs boson discovery\n",
    "\n",
    "The discovery of the Higgs boson in July 2012 marked a fundamental breakthrough in particle physics. The Higgs boson particle was discovered through experiments at the Large Hadron Collider at CERN, by colliding beams of protons at high energy. A key challenge in analyzing the results of these experiments is to differentiate between collisions that produce Higgs bosons and collisions that produce only background noise. \n",
    "\n",
    "### Data description\n",
    "\n",
    "You are provided with data from Monte-Carlo simulations of collisions of particles in a particle collider experiment. The training set is available in `Higgs_train.csv` and the test set is in `Higgs_test.csv`. Each row in these files corresponds to a particle collision described by 28 features (columns 1-28), of which the first 21 features are kinematic properties measured by the particle detectors in the accelerator, and the remaining features are derived by physicists from the first 21 features. The class label is provided in the last column, with a label of 1 indicating that the collision produces Higgs bosons (signal), and a label of 0 indicating that the collision produces other particles (background).\n",
    "\n",
    "The data set provided to you is a small subset of the HIGGS data set in the UCI machine learning repository. The following paper contains further details about the data set and the predictors used: [Baldi et al., Nature Communications 5, 2014](https://www.nature.com/articles/ncomms5308).\n",
    "\n",
    "### Loading the data\n",
    "\n",
    "Run the following cell to load the data. Do not modify this code. We need to ensure everyone has the exact same arrays for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our data contains 5,000 training samples and 5,000 test samples.\n",
      "\n",
      "TRAINING DATA INFORMATION:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 29 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   lepton pT                 5000 non-null   float64\n",
      " 1   lepton eta                5000 non-null   float64\n",
      " 2   lepton phi                5000 non-null   float64\n",
      " 3   missing energy magnitude  5000 non-null   float64\n",
      " 4   missing energy phi        5000 non-null   float64\n",
      " 5   jet 1 pt                  5000 non-null   float64\n",
      " 6   jet 1 eta                 5000 non-null   float64\n",
      " 7   jet 1 phi                 5000 non-null   float64\n",
      " 8   jet 1 b-tag               5000 non-null   float64\n",
      " 9   jet 2 pt                  5000 non-null   float64\n",
      " 10  jet 2 eta                 5000 non-null   float64\n",
      " 11  jet 2 phi                 5000 non-null   float64\n",
      " 12  jet 2 b-tag               5000 non-null   float64\n",
      " 13  jet 3 pt                  5000 non-null   float64\n",
      " 14  jet 3 eta                 5000 non-null   float64\n",
      " 15  jet 3 phi                 5000 non-null   float64\n",
      " 16  jet 3 b-tag               5000 non-null   float64\n",
      " 17  jet 4 pt                  5000 non-null   float64\n",
      " 18  jet 4 eta                 5000 non-null   float64\n",
      " 19  jet 4 phi                 5000 non-null   float64\n",
      " 20  jet 4 b-tag               5000 non-null   float64\n",
      " 21  m_jj                      5000 non-null   float64\n",
      " 22  m_jjj                     5000 non-null   float64\n",
      " 23  m_lv                      5000 non-null   float64\n",
      " 24  m_jlv                     5000 non-null   float64\n",
      " 25  m_bb                      5000 non-null   float64\n",
      " 26  m_wbb                     5000 non-null   float64\n",
      " 27  m_wwbb                    5000 non-null   float64\n",
      " 28  class                     5000 non-null   float64\n",
      "dtypes: float64(29)\n",
      "memory usage: 1.1 MB\n",
      "\n",
      "TRAINING DATA HEAD:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>jet 2 eta</th>\n",
       "      <th>jet 2 phi</th>\n",
       "      <th>jet 2 b-tag</th>\n",
       "      <th>jet 3 pt</th>\n",
       "      <th>jet 3 eta</th>\n",
       "      <th>jet 3 phi</th>\n",
       "      <th>jet 3 b-tag</th>\n",
       "      <th>jet 4 pt</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.377</td>\n",
       "      <td>-1.5800</td>\n",
       "      <td>-1.7100</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.114</td>\n",
       "      <td>1.250</td>\n",
       "      <td>0.620</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>-0.667</td>\n",
       "      <td>2.21</td>\n",
       "      <td>1.280</td>\n",
       "      <td>-1.190</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.110</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.522</td>\n",
       "      <td>1.320</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.360</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.080</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.707</td>\n",
       "      <td>0.0876</td>\n",
       "      <td>-0.4000</td>\n",
       "      <td>0.919</td>\n",
       "      <td>-1.230</td>\n",
       "      <td>1.170</td>\n",
       "      <td>-0.553</td>\n",
       "      <td>0.886</td>\n",
       "      <td>2.17</td>\n",
       "      <td>1.300</td>\n",
       "      <td>0.7620</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.459</td>\n",
       "      <td>1.020</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.829</td>\n",
       "      <td>0.992</td>\n",
       "      <td>1.160</td>\n",
       "      <td>2.220</td>\n",
       "      <td>1.190</td>\n",
       "      <td>0.938</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.617</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>-1.3500</td>\n",
       "      <td>1.150</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.377</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.060</td>\n",
       "      <td>-0.0194</td>\n",
       "      <td>1.110</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.470</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-1.060</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.490</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.030</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.917</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.851</td>\n",
       "      <td>-0.3810</td>\n",
       "      <td>-0.0713</td>\n",
       "      <td>1.470</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.620</td>\n",
       "      <td>0.1240</td>\n",
       "      <td>1.180</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.916</td>\n",
       "      <td>2.55</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.796</td>\n",
       "      <td>-1.520</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.200</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1.350</td>\n",
       "      <td>1.460</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.954</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.768</td>\n",
       "      <td>-0.6920</td>\n",
       "      <td>-0.0402</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.397</td>\n",
       "      <td>-0.874</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>1.320</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.730</td>\n",
       "      <td>-0.758</td>\n",
       "      <td>-1.120</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.848</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.502</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.919</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lepton pT  lepton eta  lepton phi  missing energy magnitude  missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  jet 2 pt  jet 2 eta  jet 2 phi  jet 2 b-tag  jet 3 pt  jet 3 eta  jet 3 phi  jet 3 b-tag  jet 4 pt  jet 4 eta  jet 4 phi  jet 4 b-tag   m_jj  m_jjj   m_lv  m_jlv   m_bb  m_wbb  m_wwbb  class\n",
       "0      0.377     -1.5800     -1.7100                     0.991               0.114     1.250      0.620     -1.480         2.17     0.754     0.7750     -0.667         2.21     1.280     -1.190      0.505         0.00     1.110     -0.464      0.397         0.00  0.522  1.320  0.982  1.360  0.965  1.310   1.080    1.0\n",
       "1      0.707      0.0876     -0.4000                     0.919              -1.230     1.170     -0.553      0.886         2.17     1.300     0.7620     -1.060         2.21     0.607      0.459      1.020         0.00     0.497      0.956      0.236         0.00  0.440  0.829  0.992  1.160  2.220  1.190   0.938    1.0\n",
       "2      0.617      0.2660     -1.3500                     1.150               1.040     0.955      0.377     -0.148         0.00     1.060    -0.0194      1.110         0.00     1.470      0.205     -1.060         2.55     1.490     -0.398     -0.542         0.00  1.020  1.030  0.986  0.928  1.370  0.982   0.917    1.0\n",
       "3      0.851     -0.3810     -0.0713                     1.470              -0.795     0.692      0.883      0.497         0.00     1.620     0.1240      1.180         1.11     1.290      0.160     -0.916         2.55     0.945      0.796     -1.520         0.00  1.200  1.100  0.987  1.350  1.460  0.995   0.954    1.0\n",
       "4      0.768     -0.6920     -0.0402                     0.615               0.144     0.749      0.397     -0.874         0.00     1.150     0.1270      1.320         2.21     0.730     -0.758     -1.120         0.00     0.848      0.107      0.502         1.55  0.922  0.864  0.983  1.370  0.601  0.919   0.957    0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING DATA SUMMARY STATISTICS:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lepton pT</th>\n",
       "      <th>lepton eta</th>\n",
       "      <th>lepton phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>jet 1 pt</th>\n",
       "      <th>jet 1 eta</th>\n",
       "      <th>jet 1 phi</th>\n",
       "      <th>jet 1 b-tag</th>\n",
       "      <th>jet 2 pt</th>\n",
       "      <th>jet 2 eta</th>\n",
       "      <th>jet 2 phi</th>\n",
       "      <th>jet 2 b-tag</th>\n",
       "      <th>jet 3 pt</th>\n",
       "      <th>jet 3 eta</th>\n",
       "      <th>jet 3 phi</th>\n",
       "      <th>jet 3 b-tag</th>\n",
       "      <th>jet 4 pt</th>\n",
       "      <th>jet 4 eta</th>\n",
       "      <th>jet 4 phi</th>\n",
       "      <th>jet 4 b-tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.978645</td>\n",
       "      <td>-0.014280</td>\n",
       "      <td>-0.018956</td>\n",
       "      <td>1.005793</td>\n",
       "      <td>0.002528</td>\n",
       "      <td>0.980390</td>\n",
       "      <td>0.025014</td>\n",
       "      <td>-0.007104</td>\n",
       "      <td>0.993678</td>\n",
       "      <td>0.988659</td>\n",
       "      <td>-0.010310</td>\n",
       "      <td>-0.006926</td>\n",
       "      <td>1.006922</td>\n",
       "      <td>0.997004</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>1.011994</td>\n",
       "      <td>0.982806</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>1.007810</td>\n",
       "      <td>1.038431</td>\n",
       "      <td>1.027201</td>\n",
       "      <td>1.054719</td>\n",
       "      <td>1.023094</td>\n",
       "      <td>0.958464</td>\n",
       "      <td>1.033432</td>\n",
       "      <td>0.960494</td>\n",
       "      <td>0.524600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.547025</td>\n",
       "      <td>1.011927</td>\n",
       "      <td>0.997945</td>\n",
       "      <td>0.591907</td>\n",
       "      <td>1.003337</td>\n",
       "      <td>0.463677</td>\n",
       "      <td>1.002018</td>\n",
       "      <td>1.014559</td>\n",
       "      <td>1.028920</td>\n",
       "      <td>0.476462</td>\n",
       "      <td>1.007983</td>\n",
       "      <td>1.002177</td>\n",
       "      <td>1.045206</td>\n",
       "      <td>0.471681</td>\n",
       "      <td>1.007824</td>\n",
       "      <td>0.999656</td>\n",
       "      <td>1.200416</td>\n",
       "      <td>0.497681</td>\n",
       "      <td>1.007999</td>\n",
       "      <td>1.008904</td>\n",
       "      <td>1.400846</td>\n",
       "      <td>0.619460</td>\n",
       "      <td>0.353984</td>\n",
       "      <td>0.173243</td>\n",
       "      <td>0.427141</td>\n",
       "      <td>0.495720</td>\n",
       "      <td>0.352966</td>\n",
       "      <td>0.306057</td>\n",
       "      <td>0.499444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.275000</td>\n",
       "      <td>-2.410000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>-2.920000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.198000</td>\n",
       "      <td>-2.910000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>-2.720000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>-1.740000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151000</td>\n",
       "      <td>0.443000</td>\n",
       "      <td>0.339000</td>\n",
       "      <td>0.371000</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.413000</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.587000</td>\n",
       "      <td>-0.764250</td>\n",
       "      <td>-0.877500</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>-0.870000</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>-0.659250</td>\n",
       "      <td>-0.885000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666000</td>\n",
       "      <td>-0.699000</td>\n",
       "      <td>-0.859500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664750</td>\n",
       "      <td>-0.679250</td>\n",
       "      <td>-0.858000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.619000</td>\n",
       "      <td>-0.707250</td>\n",
       "      <td>-0.869250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.798750</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.672000</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.772750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.846000</td>\n",
       "      <td>-0.009305</td>\n",
       "      <td>-0.016050</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>-0.023500</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>0.891000</td>\n",
       "      <td>-0.004800</td>\n",
       "      <td>-0.030700</td>\n",
       "      <td>1.110000</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.877000</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>-0.004700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.898000</td>\n",
       "      <td>0.957000</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.922000</td>\n",
       "      <td>0.868000</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.877500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.725500</td>\n",
       "      <td>0.837000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>0.716000</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>1.192500</td>\n",
       "      <td>0.692250</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>1.232500</td>\n",
       "      <td>0.717000</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>1.140000</td>\n",
       "      <td>1.060000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.330000</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>6.260000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>4.190000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.210000</td>\n",
       "      <td>4.630000</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>2.550000</td>\n",
       "      <td>5.770000</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>1.740000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>5.740000</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>6.220000</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>4.320000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         lepton pT   lepton eta   lepton phi  missing energy magnitude  missing energy phi     jet 1 pt    jet 1 eta    jet 1 phi  jet 1 b-tag     jet 2 pt    jet 2 eta    jet 2 phi  jet 2 b-tag     jet 3 pt    jet 3 eta    jet 3 phi  jet 3 b-tag     jet 4 pt    jet 4 eta    jet 4 phi  jet 4 b-tag         m_jj        m_jjj         m_lv        m_jlv         m_bb        m_wbb       m_wwbb        class\n",
       "count  5000.000000  5000.000000  5000.000000               5000.000000         5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000\n",
       "mean      0.978645    -0.014280    -0.018956                  1.005793            0.002528     0.980390     0.025014    -0.007104     0.993678     0.988659    -0.010310    -0.006926     1.006922     0.997004     0.018817     0.003952     1.011994     0.982806     0.005201     0.003349     1.007810     1.038431     1.027201     1.054719     1.023094     0.958464     1.033432     0.960494     0.524600\n",
       "std       0.547025     1.011927     0.997945                  0.591907            1.003337     0.463677     1.002018     1.014559     1.028920     0.476462     1.007983     1.002177     1.045206     0.471681     1.007824     0.999656     1.200416     0.497681     1.007999     1.008904     1.400846     0.619460     0.353984     0.173243     0.427141     0.495720     0.352966     0.306057     0.499444\n",
       "min       0.275000    -2.410000    -1.740000                  0.010000           -1.740000     0.170000    -2.920000    -1.740000     0.000000     0.198000    -2.910000    -1.740000     0.000000     0.265000    -2.720000    -1.740000     0.000000     0.366000    -2.500000    -1.740000     0.000000     0.151000     0.443000     0.339000     0.371000     0.079500     0.413000     0.452000     0.000000\n",
       "25%       0.587000    -0.764250    -0.877500                  0.581000           -0.870000     0.676000    -0.659250    -0.885000     0.000000     0.666000    -0.699000    -0.859500     0.000000     0.664750    -0.679250    -0.858000     0.000000     0.619000    -0.707250    -0.869250     0.000000     0.798750     0.850000     0.986000     0.768000     0.672000     0.826000     0.772750     0.000000\n",
       "50%       0.846000    -0.009305    -0.016050                  0.903500            0.001300     0.891000     0.049500    -0.023500     1.090000     0.891000    -0.004800    -0.030700     1.110000     0.899500     0.045700     0.018800     0.000000     0.877000     0.012900    -0.004700     0.000000     0.898000     0.957000     0.990000     0.922000     0.868000     0.952000     0.877500     1.000000\n",
       "75%       1.220000     0.725500     0.837000                  1.300000            0.866000     1.160000     0.716000     0.894000     2.170000     1.192500     0.692250     0.855500     2.210000     1.232500     0.717000     0.855000     2.550000     1.220000     0.719000     0.859000     3.100000     1.030000     1.090000     1.030000     1.160000     1.120000     1.140000     1.060000     1.000000\n",
       "max       5.330000     2.430000     1.740000                  6.260000            1.740000     4.190000     2.960000     1.740000     2.170000     4.800000     2.910000     1.740000     2.210000     4.630000     2.730000     1.740000     2.550000     5.770000     2.490000     1.740000     3.100000    10.600000     5.740000     3.940000     6.220000     5.080000     4.320000     3.500000     1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# RUN THIS CELL\n",
    "# DO NOT MODIFY THIS CODE\n",
    "\n",
    "data_train = pd.read_csv(\"data/Higgs_train.csv\")\n",
    "data_test = pd.read_csv(\"data/Higgs_test.csv\")\n",
    "\n",
    "print(\n",
    "    f\"Our data contains {len(data_train):,} training samples \"\n",
    "    f\"and {len(data_test):,} test samples.\\n\"\n",
    ")\n",
    "\n",
    "print(\"TRAINING DATA INFORMATION:\\n\")\n",
    "data_train.info()\n",
    "\n",
    "print(\"\\nTRAINING DATA HEAD:\")\n",
    "display(data_train.head())\n",
    "\n",
    "print(\"\\nTRAINING DATA SUMMARY STATISTICS:\")\n",
    "display(data_train.describe())\n",
    "\n",
    "# Split dataframe into X and y numpy arrays\n",
    "X_train = data_train.iloc[:, data_train.columns != \"class\"].values\n",
    "y_train = data_train[\"class\"].values\n",
    "X_test = data_test.iloc[:, data_test.columns != \"class\"].values\n",
    "y_test = data_test[\"class\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"part1\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 1: Decision Tree [21 pts]</div> \n",
    "    \n",
    "[Return to contents](#contents)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**1.1** In this problem, we will observe how both tree-depth and cross-validation affect our ability to accurately model data. Specifically, for each tree depth from 1 to 20 (inclusive):\n",
    "\n",
    "- Fit a decision tree to the entire **training** set.\n",
    "\n",
    "- Evaluate on the entire **training** set (i.e., `.score(...)`), while storing the scores in a variable named `train_scores`.\n",
    "\n",
    "- Perform 5-fold cross-validation with the entire **training** set using [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html). Store the mean validation score and the validation standard deviation  in variables named `cvmeans` and `cvstds` respectively.\n",
    "</div>\n",
    "\n",
    "_Points:_ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "tree_depth_start, tree_depth_end = 1,21\n",
    "train_scores=[]\n",
    "cvmeans=[]\n",
    "cvstds=[]\n",
    "\n",
    "for i in range(tree_depth_start, tree_depth_end):\n",
    "    tree1_1 = DecisionTreeClassifier(criterion='gini',max_depth=i).fit(X_train,y_train)\n",
    "    train_scores.append(tree1_1.score(X_train,y_train))\n",
    "    scores=cross_val_score(estimator=tree1_1,X=X_train,y=y_train,cv=5, n_jobs= -1)\n",
    "    cvmeans.append(scores.mean())\n",
    "    cvstds.append(np.std(scores))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5953999999999999,\n",
       " 0.6172,\n",
       " 0.6212,\n",
       " 0.6396,\n",
       " 0.6426000000000001,\n",
       " 0.6382000000000001,\n",
       " 0.6336,\n",
       " 0.6260000000000001,\n",
       " 0.618,\n",
       " 0.6098,\n",
       " 0.6138,\n",
       " 0.6060000000000001,\n",
       " 0.5953999999999999,\n",
       " 0.5962,\n",
       " 0.5906,\n",
       " 0.5902,\n",
       " 0.5921999999999998,\n",
       " 0.5888,\n",
       " 0.585,\n",
       " 0.5871999999999999]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvmeans.index(max(cvmeans))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1.1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q1.1 results: All test cases passed!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "    \n",
    "**1.2** Now that we have `train_scores`, `cvmeans`, and `cvstds`, let's plot them. Generate 2 plots, both showing (a) the non-cross-validation training scores, (b) the mean validation scores, and (c) a shaded region that illustrates the +/-2 standard deviation validation bounds for each tree depth. The content and formatting of these 2 plots should be identical, EXCEPT in one plot set the limits on the y-axis to focus on the validation performance. Remember to label and title each plot appropropriately.\n",
    "\n",
    "**HINT:** You can use `plt.fill_between(...)` to easily generate the shaded region in your plots.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotswith labels\n",
    "ax.plot(list(range(tree_depth_start,tree_depth_end)),train_scores, label='train score')\n",
    "ax.plot(list(range(tree_depth_start,tree_depth_end)),cvmeans, label='cvmeans')\n",
    "ax.plot(list(range(tree_depth_start,tree_depth_end)),cvstds, label='cvstds')\n",
    "ax.fill_between(list(range(tree_depth_start,tree_depth_end)),cvmeans-2*np.array(cvstds),np.array(cvmeans)+2*np.array(cvstds),color='slategrey',alpha=0.2)\n",
    "# Set the plot labels and legends\n",
    "ax.set_xlabel('Tree Depth')\n",
    "ax.legend(loc = 'best')\n",
    "plt.show();\n",
    "#use fill_between for the std.devs\n",
    "#for limiting y, set ylim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotswith labels\n",
    "ax.plot(list(range(tree_depth_start,tree_depth_end)),train_scores, label='train score')\n",
    "ax.plot(list(range(tree_depth_start,tree_depth_end)),cvmeans, label='cvmeans')\n",
    "ax.plot(list(range(tree_depth_start,tree_depth_end)),cvstds, label='cvstds')\n",
    "ax.fill_between(list(range(tree_depth_start,tree_depth_end)),cvmeans-2*np.array(cvstds),np.array(cvmeans)+2*np.array(cvstds),color='slategrey',alpha=0.2)\n",
    "# Set the plot labels and legends\n",
    "ax.set_ylim(0.57,0.67)\n",
    "ax.set_xlabel('Tree Depth')\n",
    "ax.legend(loc = 'best')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**1.3** Using the cross-validation experiments from above, select a depth you deem most appropriate for using on future, unseen data, store it in `best_cv_depth`, and **justify your choice**. Then, using this depth, fit a new decision tree on the entire training data and store the train and test accuracies in `best_cv_tree_train_score` and `best_cv_tree_test_score`, respectively, which we will refer to in later questions.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I would choose depth 5, because it is the depth after which although the train score without cross-validation continues to increase, the mean cross-validation decreases._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# choose best depth after a qualitative assessment of our plots\n",
    "best_cv_depth = cvmeans.index(max(cvmeans))+1\n",
    "\n",
    "tree1_3 = DecisionTreeClassifier(criterion='gini',max_depth=best_cv_depth).fit(X_train,y_train)\n",
    "best_cv_tree_train_score = tree1_3.score(X_train,y_train)\n",
    "best_cv_tree_test_score = tree1_3.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tree of max-depth 5 trained on the full training set, achieves the following accuracy scores:\n",
      "\n",
      "\ttrain\t0.6812\n",
      "\tTEST\t0.6478\n"
     ]
    }
   ],
   "source": [
    "# print model results summary\n",
    "print(\n",
    "    \"The tree of max-depth {} trained on the \"\n",
    "    \"full training set, achieves the following accuracy scores:\"\n",
    "    \"\\n\\n\\ttrain\\t{:.4f}\\n\\tTEST\\t{:.4f}\".format(\n",
    "        best_cv_depth,\n",
    "        best_cv_tree_train_score,\n",
    "        best_cv_tree_test_score,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**1.4** In terms of the bias-variance tradeoff, how does limiting tree depth avoid over-fitting? What is one downside of limiting the tree depth?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_As tree depth increases,the tree fits more closely to the training data by dividing the observations with additional decision boundaries, but it can go to the point of fitting each individual observation, which is not useful for creating a model. Limiting depth prevents this from happening, but may also lead to the model not capturing non-linear boundaries between classes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part2\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 2: Bagging [20 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**2.1** Based on your results from [Question 1](#part1solutions), choose a tree depth that WILL overfit the training set. What evidence leads you to believe that this depth overfits? Assign your choice to a variable named `tree_depth`. (You may want to explore different settings for this value in the problems below. However, be certain that your final choice and rationale is based on your results from Question 1.)\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_I'm choosing a tree depth of 20. I believe this depth overfits because while the training score has improved upon tree depth=5, the mean cross validation score has decreased, which suggests that it has overfit to the train data and performs more poorly when applied to the validation datasets._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set overfitting tree_depth based on provided rationale \n",
    "tree_depth = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**2.2** Here we will use the `tree_depth` chosen in Question 2.1 to generate 55 boostrapped sets of decision tree predictions for both the training and test data. To accomplish this:\n",
    "\n",
    "- Using a random seed of 0, call the provided `bagger` function to return your bootstrapped results.\n",
    "- Store your returned results as: \n",
    "  1. `bagging_train_df`: a dataframe containing your training data predictions (see the \"required dataframe structure\" below)\n",
    "  2. `bagging_test_df`: a dataframe containing your test data predictions\n",
    "  3. `bagging_models_list`: a list containing your 55 fitted model objects (i.e. fitted estimators)\n",
    "- Finally, display the heads of both dataframes.\n",
    "\n",
    "**NOTE:** There is no need to do anything with your `bagging_models_list` list yet. It will not be used until later in [Question 3.2](#part3).\n",
    "\n",
    "**DATAFRAME STRUCTURE:** The training and test prediction results of your bootstraps are returned by the `bagger` function as dataframes formatted like the example shown below. Each row represents one observation (from either the training or test set depending on the dataframe), and each column represents one bootstrapped result. The values stored in the dataframe are the bootstrapped predictions for each observation as illustrated below.\n",
    "\n",
    "An example of the `bagging_train_df` and `bagging_test_df` dataframes would be:\n",
    "\n",
    "|     |bootstrap model 1|bootstrap model 2|...|bootstrap model 55|  \n",
    "| --- | --- | --- | --- | --- |\n",
    "|0| 0 | 1|... |0|\n",
    "|1| 1| 1|... |0|\n",
    "|2| 0| 0|... |1|\n",
    "|...| ...| ...|... |... |\n",
    "| $n$-1 | 0| 0|... |1|\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def bagger(\n",
    "    n_trees: int,\n",
    "    tree_depth: int,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    random_seed: int = 0,\n",
    ") -> (pd.DataFrame, pd.DataFrame, list):\n",
    "    \"\"\"Generate boostrapped DecisionTreeClassifier predictions\n",
    "    \n",
    "    Function fits bootstrapped DecisionTreeClassifier models\n",
    "    and returns training and test predictions for each of those\n",
    "    bootstrapped models, along with the fitted model objects as\n",
    "    described in Question 2.2 question text.\n",
    "    \n",
    "    :param n_trees: int, number of bootstrapped decision trees\n",
    "    :param tree_depth: int, maximum tree depth\n",
    "    :param X_train: np.ndarray, training X observations\n",
    "    :param y_train: np.ndarray, training y observations\n",
    "    :param X_test: np.ndarray, test X observations\n",
    "    :param random_seed: int, random seed used to set np.random.seed\n",
    "                        to ensure replicable results (default=0)\n",
    "    \n",
    "    :returns: (pd.DataFrame, pd.DataFrame, list), tuple containing 3\n",
    "              objects, (1) bagging_train_df dataframe\n",
    "              as described in Q2.2 question text, (2) bagging_test_df\n",
    "              dataframe as described in Q2.2, and (3) bagging_models_list\n",
    "              containing every trained DecisionTreeClassifier model\n",
    "              object (i.e. estimator), one estimator for each bootstrap\n",
    "              (you will need this list later in Q3.2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # set random seed for replicable results\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # instantiate arrays and list for storing results\n",
    "    bagging_train = np.zeros((X_train.shape[0], n_trees)).astype(int)\n",
    "    bagging_test = np.zeros((X_test.shape[0], n_trees)).astype(int)\n",
    "\n",
    "    bagging_models_list = []\n",
    "\n",
    "    # perform n bootstraps\n",
    "    for i in range(n_trees):\n",
    "        # generate bootstrapped model\n",
    "        bootstrapped_X, bootstrapped_y = resample(X_train, y_train)\n",
    "        fitted_model = DecisionTreeClassifier(\n",
    "            max_depth=tree_depth\n",
    "        ).fit(bootstrapped_X, bootstrapped_y)\n",
    "        bagging_models_list.append(fitted_model)\n",
    "\n",
    "        # predict on full training and test sets and store\n",
    "        # results to arrays\n",
    "        bagging_train[:,i] = fitted_model.predict(X_train)\n",
    "        bagging_test[:,i] = fitted_model.predict(X_test)\n",
    "    \n",
    "    # convert arrays to pandas dataframes as required\n",
    "    bagging_train_df = pd.DataFrame(\n",
    "        bagging_train[:, :],\n",
    "        columns=[f\"model{x}\" for x in range(n_trees)],\n",
    "    )\n",
    "    bagging_test_df = pd.DataFrame(\n",
    "        bagging_test[:, :],\n",
    "        columns=[f\"model{x}\" for x in range(n_trees)],\n",
    "    )\n",
    "    \n",
    "    return bagging_train_df, bagging_test_df, bagging_models_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify number of required bootrapped trees\n",
    "n_trees = 55 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bagger2_2=bagger(n_trees,tree_depth,X_train,y_train,X_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model0</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>model5</th>\n",
       "      <th>model6</th>\n",
       "      <th>model7</th>\n",
       "      <th>model8</th>\n",
       "      <th>model9</th>\n",
       "      <th>model10</th>\n",
       "      <th>model11</th>\n",
       "      <th>model12</th>\n",
       "      <th>model13</th>\n",
       "      <th>model14</th>\n",
       "      <th>model15</th>\n",
       "      <th>model16</th>\n",
       "      <th>model17</th>\n",
       "      <th>model18</th>\n",
       "      <th>model19</th>\n",
       "      <th>model20</th>\n",
       "      <th>model21</th>\n",
       "      <th>model22</th>\n",
       "      <th>model23</th>\n",
       "      <th>model24</th>\n",
       "      <th>model25</th>\n",
       "      <th>model26</th>\n",
       "      <th>model27</th>\n",
       "      <th>model28</th>\n",
       "      <th>model29</th>\n",
       "      <th>model30</th>\n",
       "      <th>model31</th>\n",
       "      <th>model32</th>\n",
       "      <th>model33</th>\n",
       "      <th>model34</th>\n",
       "      <th>model35</th>\n",
       "      <th>model36</th>\n",
       "      <th>model37</th>\n",
       "      <th>model38</th>\n",
       "      <th>model39</th>\n",
       "      <th>model40</th>\n",
       "      <th>model41</th>\n",
       "      <th>model42</th>\n",
       "      <th>model43</th>\n",
       "      <th>model44</th>\n",
       "      <th>model45</th>\n",
       "      <th>model46</th>\n",
       "      <th>model47</th>\n",
       "      <th>model48</th>\n",
       "      <th>model49</th>\n",
       "      <th>model50</th>\n",
       "      <th>model51</th>\n",
       "      <th>model52</th>\n",
       "      <th>model53</th>\n",
       "      <th>model54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model0  model1  model2  model3  model4  model5  model6  model7  model8  model9  model10  model11  model12  model13  model14  model15  model16  model17  model18  model19  model20  model21  model22  model23  model24  model25  model26  model27  model28  model29  model30  model31  model32  model33  model34  model35  model36  model37  model38  model39  model40  model41  model42  model43  model44  model45  model46  model47  model48  model49  model50  model51  model52  model53  model54\n",
       "0          1       1       1       1       0       0       1       0       1       1        1        1        0        1        1        1        1        0        1        0        1        1        1        0        1        1        1        1        1        1        1        0        1        1        1        1        1        1        0        0        1        1        1        1        1        1        1        0        1        1        1        1        1        1        1\n",
       "1          0       1       1       1       1       1       1       1       0       0        1        1        1        1        0        1        1        1        1        1        1        1        1        1        1        0        1        0        0        0        1        0        1        1        1        1        1        0        1        1        1        1        1        1        1        1        1        0        0        1        1        1        0        0        1\n",
       "2          1       1       1       1       1       1       1       1       1       1        1        1        1        1        1        0        0        1        1        0        1        1        0        1        0        1        1        1        1        1        0        1        1        1        1        1        1        1        1        0        1        1        1        1        1        0        0        1        1        1        1        0        1        0        1\n",
       "3          1       1       1       1       1       1       1       1       0       0        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        0        1        1        1        1        0        1        1        1        0        0        0        1        1        0        1        1        1        1        1        1        1        1\n",
       "4          0       0       0       0       0       0       0       1       1       0        0        0        0        0        0        1        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        1        0        0        0        0        0        0        1        0        1        0        0        0        0        0        0        1        0        1        1        0        0\n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "4995       1       1       1       1       1       1       1       1       0       1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        0        1        1        1        1        1        1        0        1        0        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        0        1        1        1\n",
       "4996       1       1       1       1       1       1       1       1       1       0        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        0        1        1        1        1        1        1        1        1        1        1        1        1        1        0        1        1        1        1        1        0        1        1\n",
       "4997       1       0       1       1       1       1       1       1       0       1        1        1        1        1        1        0        1        1        1        1        1        1        1        1        1        1        1        0        1        1        1        0        1        1        1        0        1        1        1        1        0        1        1        0        1        1        1        1        1        1        1        1        1        1        1\n",
       "4998       1       0       1       1       1       1       1       1       1       1        1        1        0        1        1        1        0        1        1        1        1        1        1        1        1        0        0        1        1        1        1        1        1        1        1        0        1        1        1        1        1        1        1        1        1        1        1        1        0        1        1        1        1        1        1\n",
       "4999       1       1       1       1       1       1       1       1       1       1        1        1        1        1        1        1        1        1        1        1        0        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        0        1        1        1        1        1        1        1        1        1\n",
       "\n",
       "[5000 rows x 55 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_train_df=bagger2_2[0]\n",
    "bagging_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model0</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>model5</th>\n",
       "      <th>model6</th>\n",
       "      <th>model7</th>\n",
       "      <th>model8</th>\n",
       "      <th>model9</th>\n",
       "      <th>model10</th>\n",
       "      <th>model11</th>\n",
       "      <th>model12</th>\n",
       "      <th>model13</th>\n",
       "      <th>model14</th>\n",
       "      <th>model15</th>\n",
       "      <th>model16</th>\n",
       "      <th>model17</th>\n",
       "      <th>model18</th>\n",
       "      <th>model19</th>\n",
       "      <th>model20</th>\n",
       "      <th>model21</th>\n",
       "      <th>model22</th>\n",
       "      <th>model23</th>\n",
       "      <th>model24</th>\n",
       "      <th>model25</th>\n",
       "      <th>model26</th>\n",
       "      <th>model27</th>\n",
       "      <th>model28</th>\n",
       "      <th>model29</th>\n",
       "      <th>model30</th>\n",
       "      <th>model31</th>\n",
       "      <th>model32</th>\n",
       "      <th>model33</th>\n",
       "      <th>model34</th>\n",
       "      <th>model35</th>\n",
       "      <th>model36</th>\n",
       "      <th>model37</th>\n",
       "      <th>model38</th>\n",
       "      <th>model39</th>\n",
       "      <th>model40</th>\n",
       "      <th>model41</th>\n",
       "      <th>model42</th>\n",
       "      <th>model43</th>\n",
       "      <th>model44</th>\n",
       "      <th>model45</th>\n",
       "      <th>model46</th>\n",
       "      <th>model47</th>\n",
       "      <th>model48</th>\n",
       "      <th>model49</th>\n",
       "      <th>model50</th>\n",
       "      <th>model51</th>\n",
       "      <th>model52</th>\n",
       "      <th>model53</th>\n",
       "      <th>model54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      model0  model1  model2  model3  model4  model5  model6  model7  model8  model9  model10  model11  model12  model13  model14  model15  model16  model17  model18  model19  model20  model21  model22  model23  model24  model25  model26  model27  model28  model29  model30  model31  model32  model33  model34  model35  model36  model37  model38  model39  model40  model41  model42  model43  model44  model45  model46  model47  model48  model49  model50  model51  model52  model53  model54\n",
       "0          1       0       1       1       1       0       1       0       0       0        1        1        1        0        0        0        1        0        0        0        0        0        1        0        1        0        1        1        0        1        1        1        0        1        1        1        1        1        1        1        1        0        1        1        0        0        1        1        1        0        0        0        0        1        1\n",
       "1          0       1       1       0       0       1       1       1       0       0        1        1        1        1        1        0        0        1        1        1        1        0        1        0        0        0        1        0        1        1        0        1        0        0        0        1        0        1        1        0        0        0        0        0        1        1        1        1        1        1        0        0        0        0        0\n",
       "2          1       0       1       1       1       0       1       0       1       0        0        1        0        1        0        0        0        0        0        1        0        1        1        1        0        1        1        0        0        0        0        1        0        0        0        0        1        0        1        1        1        0        0        1        0        0        0        1        1        1        0        1        0        1        0\n",
       "3          1       0       1       0       1       1       0       1       1       1        1        0        1        1        0        1        1        0        1        1        1        1        1        0        1        0        0        1        1        0        1        1        1        1        0        1        1        1        0        0        1        1        1        1        1        0        1        1        1        1        1        1        1        1        1\n",
       "4          0       0       0       0       0       0       1       0       1       0        0        0        1        0        0        0        0        1        0        1        0        1        0        0        0        1        0        1        0        1        1        0        0        1        0        0        1        1        0        1        0        0        0        0        0        1        0        0        1        1        1        0        1        1        0\n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...     ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...      ...\n",
       "4995       0       1       1       1       0       0       1       1       1       1        0        0        0        0        0        1        1        0        0        1        0        0        0        1        0        1        1        1        0        1        0        0        0        1        0        0        1        0        0        1        0        0        1        1        1        1        1        0        0        1        0        1        1        0        0\n",
       "4996       0       1       0       0       1       0       0       0       0       0        1        0        0        0        0        1        0        0        1        0        0        1        0        1        1        0        0        1        0        1        0        0        1        0        1        1        0        0        0        1        0        0        0        0        0        0        1        0        0        0        0        1        0        1        0\n",
       "4997       0       0       0       0       1       0       0       0       0       1        1        0        0        1        0        1        1        0        0        1        0        0        1        0        1        0        0        0        0        0        0        0        0        0        1        1        0        0        0        1        0        0        0        1        0        1        0        0        0        0        0        0        1        1        0\n",
       "4998       0       0       0       0       0       1       1       0       0       0        1        0        0        0        0        1        0        1        1        1        0        1        0        1        0        1        0        1        0        1        1        0        0        1        1        0        0        0        0        1        0        1        0        0        1        1        1        0        0        1        1        0        0        1        0\n",
       "4999       1       0       1       1       0       0       1       0       1       0        0        1        0        0        1        0        1        1        1        0        1        1        0        1        1        0        0        1        0        1        0        0        1        0        1        0        0        0        1        1        1        1        1        0        1        1        1        1        0        0        1        0        0        0        1\n",
       "\n",
       "[5000 rows x 55 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_test_df=bagger2_2[1]\n",
    "bagging_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model0</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>model5</th>\n",
       "      <th>model6</th>\n",
       "      <th>model7</th>\n",
       "      <th>model8</th>\n",
       "      <th>model9</th>\n",
       "      <th>model10</th>\n",
       "      <th>model11</th>\n",
       "      <th>model12</th>\n",
       "      <th>model13</th>\n",
       "      <th>model14</th>\n",
       "      <th>model15</th>\n",
       "      <th>model16</th>\n",
       "      <th>model17</th>\n",
       "      <th>model18</th>\n",
       "      <th>model19</th>\n",
       "      <th>model20</th>\n",
       "      <th>model21</th>\n",
       "      <th>model22</th>\n",
       "      <th>model23</th>\n",
       "      <th>model24</th>\n",
       "      <th>model25</th>\n",
       "      <th>model26</th>\n",
       "      <th>model27</th>\n",
       "      <th>model28</th>\n",
       "      <th>model29</th>\n",
       "      <th>model30</th>\n",
       "      <th>model31</th>\n",
       "      <th>model32</th>\n",
       "      <th>model33</th>\n",
       "      <th>model34</th>\n",
       "      <th>model35</th>\n",
       "      <th>model36</th>\n",
       "      <th>model37</th>\n",
       "      <th>model38</th>\n",
       "      <th>model39</th>\n",
       "      <th>model40</th>\n",
       "      <th>model41</th>\n",
       "      <th>model42</th>\n",
       "      <th>model43</th>\n",
       "      <th>model44</th>\n",
       "      <th>model45</th>\n",
       "      <th>model46</th>\n",
       "      <th>model47</th>\n",
       "      <th>model48</th>\n",
       "      <th>model49</th>\n",
       "      <th>model50</th>\n",
       "      <th>model51</th>\n",
       "      <th>model52</th>\n",
       "      <th>model53</th>\n",
       "      <th>model54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model0  model1  model2  model3  model4  model5  model6  model7  model8  model9  model10  model11  model12  model13  model14  model15  model16  model17  model18  model19  model20  model21  model22  model23  model24  model25  model26  model27  model28  model29  model30  model31  model32  model33  model34  model35  model36  model37  model38  model39  model40  model41  model42  model43  model44  model45  model46  model47  model48  model49  model50  model51  model52  model53  model54\n",
       "0       1       1       1       1       0       0       1       0       1       1        1        1        0        1        1        1        1        0        1        0        1        1        1        0        1        1        1        1        1        1        1        0        1        1        1        1        1        1        0        0        1        1        1        1        1        1        1        0        1        1        1        1        1        1        1\n",
       "1       0       1       1       1       1       1       1       1       0       0        1        1        1        1        0        1        1        1        1        1        1        1        1        1        1        0        1        0        0        0        1        0        1        1        1        1        1        0        1        1        1        1        1        1        1        1        1        0        0        1        1        1        0        0        1\n",
       "2       1       1       1       1       1       1       1       1       1       1        1        1        1        1        1        0        0        1        1        0        1        1        0        1        0        1        1        1        1        1        0        1        1        1        1        1        1        1        1        0        1        1        1        1        1        0        0        1        1        1        1        0        1        0        1\n",
       "3       1       1       1       1       1       1       1       1       0       0        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        1        0        1        1        1        1        0        1        1        1        0        0        0        1        1        0        1        1        1        1        1        1        1        1\n",
       "4       0       0       0       0       0       0       0       1       1       0        0        0        0        0        0        1        1        0        0        0        0        0        0        0        0        0        0        0        0        0        0        0        1        1        0        0        0        0        0        0        1        0        1        0        0        0        0        0        0        1        0        1        1        0        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model0</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>model5</th>\n",
       "      <th>model6</th>\n",
       "      <th>model7</th>\n",
       "      <th>model8</th>\n",
       "      <th>model9</th>\n",
       "      <th>model10</th>\n",
       "      <th>model11</th>\n",
       "      <th>model12</th>\n",
       "      <th>model13</th>\n",
       "      <th>model14</th>\n",
       "      <th>model15</th>\n",
       "      <th>model16</th>\n",
       "      <th>model17</th>\n",
       "      <th>model18</th>\n",
       "      <th>model19</th>\n",
       "      <th>model20</th>\n",
       "      <th>model21</th>\n",
       "      <th>model22</th>\n",
       "      <th>model23</th>\n",
       "      <th>model24</th>\n",
       "      <th>model25</th>\n",
       "      <th>model26</th>\n",
       "      <th>model27</th>\n",
       "      <th>model28</th>\n",
       "      <th>model29</th>\n",
       "      <th>model30</th>\n",
       "      <th>model31</th>\n",
       "      <th>model32</th>\n",
       "      <th>model33</th>\n",
       "      <th>model34</th>\n",
       "      <th>model35</th>\n",
       "      <th>model36</th>\n",
       "      <th>model37</th>\n",
       "      <th>model38</th>\n",
       "      <th>model39</th>\n",
       "      <th>model40</th>\n",
       "      <th>model41</th>\n",
       "      <th>model42</th>\n",
       "      <th>model43</th>\n",
       "      <th>model44</th>\n",
       "      <th>model45</th>\n",
       "      <th>model46</th>\n",
       "      <th>model47</th>\n",
       "      <th>model48</th>\n",
       "      <th>model49</th>\n",
       "      <th>model50</th>\n",
       "      <th>model51</th>\n",
       "      <th>model52</th>\n",
       "      <th>model53</th>\n",
       "      <th>model54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model0  model1  model2  model3  model4  model5  model6  model7  model8  model9  model10  model11  model12  model13  model14  model15  model16  model17  model18  model19  model20  model21  model22  model23  model24  model25  model26  model27  model28  model29  model30  model31  model32  model33  model34  model35  model36  model37  model38  model39  model40  model41  model42  model43  model44  model45  model46  model47  model48  model49  model50  model51  model52  model53  model54\n",
       "0       1       0       1       1       1       0       1       0       0       0        1        1        1        0        0        0        1        0        0        0        0        0        1        0        1        0        1        1        0        1        1        1        0        1        1        1        1        1        1        1        1        0        1        1        0        0        1        1        1        0        0        0        0        1        1\n",
       "1       0       1       1       0       0       1       1       1       0       0        1        1        1        1        1        0        0        1        1        1        1        0        1        0        0        0        1        0        1        1        0        1        0        0        0        1        0        1        1        0        0        0        0        0        1        1        1        1        1        1        0        0        0        0        0\n",
       "2       1       0       1       1       1       0       1       0       1       0        0        1        0        1        0        0        0        0        0        1        0        1        1        1        0        1        1        0        0        0        0        1        0        0        0        0        1        0        1        1        1        0        0        1        0        0        0        1        1        1        0        1        0        1        0\n",
       "3       1       0       1       0       1       1       0       1       1       1        1        0        1        1        0        1        1        0        1        1        1        1        1        0        1        0        0        1        1        0        1        1        1        1        0        1        1        1        0        0        1        1        1        1        1        0        1        1        1        1        1        1        1        1        1\n",
       "4       0       0       0       0       0       0       1       0       1       0        0        0        1        0        0        0        0        1        0        1        0        1        0        0        0        1        0        1        0        1        1        0        0        1        0        0        1        1        0        1        0        0        0        0        0        1        0        0        1        1        1        0        1        1        0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display resulting dataframe heads\n",
    "display(bagging_train_df.head())\n",
    "display(bagging_test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate predictions using bagger function\n",
    "bagging_models_list=bagger2_2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145385"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_test_df.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2.2</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2.2 results: All test cases passed!"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**2.3**  Aggregate all 55 bootstrapped models to get a combined prediction for each training and test observation (i.e. predict a `1` if, and only if, a majority of the models predict that observation to be from class 1). Assign the bagging train and test accuracies to variables named `bagging_accuracy_train` and `bagging_accuracy_test`.\n",
    "\n",
    "**HINT:** You can use `np.mean(...)` to easily test for majority. If a majority of models vote 1, consider what that implies about the mean.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(df):\n",
    "    return np.mean(df, axis=1)>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate accuracy on our bagged prediction results\n",
    "bagging_accuracy_train = accuracy_score(y_train, get_prediction(bagging_train_df))\n",
    "bagging_accuracy_test = accuracy_score(y_test, get_prediction(bagging_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging 55 trees of depth-20 achieves the following accuracy scores:\n",
      "\n",
      "\ttrain\t0.9998\n",
      "\tTEST\t0.6846\n",
      "\n",
      "Our prior single depth-5 tree achieved a TEST score of 0.6478.\n"
     ]
    }
   ],
   "source": [
    "# print summary of results\n",
    "print(\n",
    "    f\"Bagging {n_trees} trees of depth-{tree_depth} achieves \"\n",
    "    f\"the following accuracy scores:\\n\\n\\ttrain\\t\"\n",
    "    f\"{bagging_accuracy_train:.4f}\\n\\tTEST\\t\"\n",
    "    f\"{bagging_accuracy_test:.4f}\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"Our prior single depth-{best_cv_depth} tree achieved a \"\n",
    "    f\"TEST score of {best_cv_tree_test_score:.4f}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2.3</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2.3 results: All test cases passed!"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.4",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**2.4** We want to know how the number of bootstraps affects our bagging ensemble's performance. Use the `running_predictions(...)` function provided below to get the model's accuracy score when using only $j$ of the bootstrapped models, where $j \\in [1, 2, 3, ..., 55]$. Using the `tree_depth` chosen in Question 2.1, make a plot that illustrates the accuracy on the training set and test set at each number of bootstraps (varying $j$ from 1 to 55). Please see the `running_predictions` signature and docstring regarding the use of the function. You should be able to use your `bagger`-generated dataframes from Q2.2 as an input to this function.\n",
    "\n",
    "On your plot, in addition to the training and test accuracies at each value $j$, also include horizontal lines for two baseline comparisons:\n",
    "\n",
    "1. The test accuracy of the best model from [Question 1](#part1solutions);\n",
    "2. The test accuracy of a single decision tree with the overfit `tree_depth` you chose in Question 2.1, trained on the full training set.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "# DO NOT MODIFY THIS CODE\n",
    "\n",
    "def running_predictions(\n",
    "    bagger_results_df: pd.DataFrame,\n",
    "    targets: np.ndarray,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Generates running accuracy of intermediate bootstraps when bagging \n",
    "    \n",
    "    Generates a series of accuracy scores calculated using the\n",
    "    running predictions of each additional bootstrapped model\n",
    "    generated using the Question 2.2 `bagger` function. For example,\n",
    "    the first accuracy in the series represents the predictive\n",
    "    accuracy of just the first bootstrapped model. The second accuracy\n",
    "    reflects the bagged accuracy of the first 2 bootstrapped models. The\n",
    "    j-th accuracy reflects the bagged accuracy of the first j\n",
    "    bootstrapped models.\n",
    "    \n",
    "    :param bagger_results_df: pd.DataFrame, a bagging results dataframe\n",
    "                              (either train or test) output from the Q2.2\n",
    "                              `bagger` function\n",
    "    :param targets: np.ndarray, 1-dimensional array of true class labels\n",
    "                    for either train or test observations (i.e y_train or\n",
    "                    y_test, whichever corresponds to the inputted\n",
    "                    bagger_results_df)             \n",
    "    :returns: pd.Series, a series of values showing the accuracy of\n",
    "              using the initial j trees to predict the targets for each\n",
    "              value of j bootstrapped models\n",
    "    \"\"\"\n",
    "    # verify that input data objects meet the requirements specified\n",
    "    # in the docstring\n",
    "    assert type(bagger_results_df)==pd.core.frame.DataFrame, (\n",
    "        \"bagger_results_df input must be a pd.DataFrame\"\n",
    "    )\n",
    "    assert type(targets)==np.ndarray, (\n",
    "        \"targets input must be an np.ndarray\"\n",
    "    )\n",
    "    assert targets.ndim==1, (\n",
    "        \"targets input np.ndarray must be one-dimensional\"\n",
    "    )\n",
    "    \n",
    "    # identify the number of bootstrapped trees in inputted bagger df\n",
    "    n_trees = bagger_results_df.shape[1]\n",
    "    \n",
    "    # calculate the running percentage of models voting 1 as each\n",
    "    # additional model is considered\n",
    "    running_percent_1s = (\n",
    "        np.cumsum(bagger_results_df, axis=1)/np.arange(1,n_trees+1)\n",
    "    )\n",
    "    \n",
    "    # predict 1 when the running average is above 0.5\n",
    "    running_conclusions = running_percent_1s > 0.5\n",
    "    \n",
    "    # check whether the running predictions match the targets\n",
    "    running_correctnesss = running_conclusions == targets.reshape(-1,1)\n",
    "    \n",
    "    # calculate and return final accuracies\n",
    "    return np.mean(running_correctnesss, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "js=list(range(1,56))\n",
    "predictions_train=[]\n",
    "predictions_test=[]\n",
    "for j in js:\n",
    "    bagger2_4=bagger(j,tree_depth,X_train,y_train,X_test)\n",
    "    predictions_train.append(np.mean(running_predictions(bagger2_4[0],y_train)))\n",
    "    predictions_test.append(np.mean(running_predictions(bagger2_4[1],y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2_4 = DecisionTreeClassifier(criterion='gini',max_depth=tree_depth).fit(X_train,y_train)\n",
    "tree2_4_test_score = tree2_4.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotswith labels\n",
    "ax.plot(js,predictions_train, label='train accuracies')\n",
    "ax.plot(js,predictions_test, label='test accuracies')\n",
    "plt.axhline(y = tree2_4_test_score,alpha=0.4, color = 'r', linestyle = '-', label='single tree - depth=20')\n",
    "plt.axhline(y = best_cv_tree_test_score,alpha=0.4, color = 'g', linestyle = '-',label='single tree - depth=5')\n",
    "\n",
    "# Set the plot labels and legends\n",
    "ax.set_xlabel('No. of Bootstrap Trees')\n",
    "ax.legend(loc = 'best')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.5",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**2.5** Referring to your graph from 2.4, compare the performance of bagging against the baseline of a single `tree_depth` tree. Explain what you see in terms of the differences between how bagging and limiting tree depth work to control overfitting.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_While the bagging model is clearly overfitting, based on the difference between the train and the test accuracies, it still performs better than the baselines with a single tree depth. This is because the single trees control overfitting by limiting tree depth, as can be seen from the superior score of the depth=5 vs depth=20 tree, whereas bagging controls overfitting by aggregating the predictions from different models that fit on random samples of observations from the original dataset, thereby accounting for variation between training and testing datasets._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part3\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 3: Random Forests [14 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "3.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<div class='exercise-r'>\n",
    "\n",
    "**3.1**  Fit a `RandomForestClassifier` to the original `X_train` data using the same tree depth and number of trees you used in Question 2.2, and set the maximum number of features to use when looking for the best split to be the square root of the total number of features. Evaluate classifier's accuracy on the training and test sets and store them in `random_forest_train_score` and `random_forest_test_score`.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest3_1=RandomForestClassifier(max_depth=tree_depth,  n_estimators=n_trees, max_features='sqrt').fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate results\n",
    "random_forest_train_score = random_forest3_1.score(X_train,y_train)\n",
    "random_forest_test_score = random_forest3_1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print results summary\n",
    "print(\n",
    "    \"The random forest of depth-{} and {} trees achieves the \"\n",
    "    \"following accuracy scores:\\n\\n\\ttrain\\t{:.4f}\\n\\tTEST\\t{:.4f}\"\n",
    "    .format(\n",
    "        tree_depth,\n",
    "        n_trees,\n",
    "        random_forest_train_score,\n",
    "        random_forest_test_score,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "3.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**3.2** Among all of the decision trees you fit in the bagging process (i.e. each of the fitted model objects stored in `bagging_models_list`), how many times is each feature used as the top/first node? How about for each tree in the random forest you just fit? Assign the answers to these questions to two pandas Series Dataframes called `top_predictors_bagging` and `top_predictors_rf`, and display them.\n",
    "\n",
    "What about the process of training the Random Forest causes this difference? What implication does this observation have on the accuracy of bagging vs. random forest?\n",
    "\n",
    "**HINT:** A decision tree's top feature is stored as `.tree_.feature[0]`. A random forest object stores its decision trees in its `.estimators_` attribute.\n",
    "    \n",
    "**IMPORTANT:** As always, your output should be easy to interpret. In this context that means construcing your DataFrames with the care, assigning appropriate column names and/or index values to best convey to the reader what the output represents.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train=pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#getting top feature for each bagging model\n",
    "bagging_predictors=[]\n",
    "for model in bagging_models_list:\n",
    "    bagging_predictors.append(model.tree_.feature[0])\n",
    "    #print(model.tree_.feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the frequency of each predictor as top predictor\n",
    "bag_predictors_count=[]\n",
    "for predictor in df_train.columns:\n",
    "    bag_predictors_count.append(bagging_predictors.count(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#getting final dataframe of top bagging predictors\n",
    "top_predictors_bagging=pd.DataFrame(bag_predictors_count,columns=['Predictor_Frequency'],index=data_train.drop(columns=['class']).columns)\n",
    "top_predictors_bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rand_forest_predictors=[]\n",
    "for tree in random_forest3_1.estimators_:\n",
    "    rand_forest_predictors.append(tree.tree_.feature[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the frequency of each predictor as top predictor\n",
    "forest_predictors_count=[]\n",
    "for predictor in df_train.columns:\n",
    "    forest_predictors_count.append(rand_forest_predictors.count(predictor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#getting final dataframe of top bagging predictors\n",
    "top_predictors_rf=pd.DataFrame(forest_predictors_count,columns=['Predictor_Frequency'],index=data_train.drop(columns=['class']).columns)\n",
    "top_predictors_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The reason for the differences between the frequency with which predictors are the top predictor in the tree for bagging and random forest is that in RF a subset of features are used to make splits, therefore if a single training feature is likely to contribute most to increased purity, its importance in the model is reduced by randomly removing it from the feature set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "3.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**3.3**: Make a Pandas DataFrame (following the expected structure shown below) of the training and test accuracy for the following models and name it `results_df`:\n",
    "\n",
    "1. Single tree with the best depth chosen by cross-validation (from Question 1)\n",
    "2. A single overfit tree trained on all data (from Question 2, using the depth you chose there)\n",
    "3. Bagging 55 such trees (from Question 2)\n",
    "4. A random forest of 55 such trees (from Question 3.1)\n",
    "\n",
    "Display your `results_df` dataframe and answer: What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance tradeoff.\n",
    "\n",
    "**NOTE:** This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.\n",
    "\n",
    "The expected structure for `results_df` is:  \n",
    "\n",
    "| classifier | training accuracy | test accuracy |\n",
    "| --- |  --- | --- |\n",
    "| single depth-$i$ tree chosen by CV | ... | ... |\n",
    "| single overfit depth-$k$ tree | ... | ... |\n",
    "| bagging 55 depth-$k$ trees | ... | ... |\n",
    "| random forest of 55 depth-$k$ trees | ... | ... |\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**3.3**: Make a Pandas DataFrame (following the expected structure shown below) of the training and test accuracy for the following models and name it `results_df`:\n",
    "\n",
    "1. Single tree with the best depth chosen by cross-validation (from Question 1)\n",
    "2. A single overfit tree trained on all data (from Question 2, using the depth you chose there)\n",
    "3. Bagging 55 such trees (from Question 2)\n",
    "4. A random forest of 55 such trees (from Question 3.1)\n",
    "\n",
    "Display your `results_df` dataframe and answer: What is the relative performance of each model on the training set? On the test set? Comment on how these relationships make sense (or don't make sense) in light of how each model treats the bias-variance tradeoff.\n",
    "\n",
    "**NOTE:** This problem should not require fitting any new models, though you may need to go back and store the accuracies from models you fit previously.\n",
    "\n",
    "The expected structure for `results_df` is:  \n",
    "\n",
    "| classifier | training accuracy | test accuracy |\n",
    "| --- |  --- | --- |\n",
    "| single depth-$i$ tree chosen by CV | ... | ... |\n",
    "| single overfit depth-$k$ tree | ... | ... |\n",
    "| bagging 55 depth-$k$ trees | ... | ... |\n",
    "| random forest of 55 depth-$k$ trees | ... | ... |\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2_4_train_score=tree2_4.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "data={'training accuracy':[best_cv_tree_train_score,tree2_4_train_score,bagging_accuracy_train,random_forest_train_score]\n",
    "           ,'test accuracy':[best_cv_tree_test_score,tree2_4_test_score,bagging_accuracy_test,random_forest_test_score]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df=pd.DataFrame(data=data,index=['single depth- 𝑖  tree chosen by CV','single overfit depth- 𝑘  tree','bagging 55 depth- 𝑘  trees','random forest of 55 depth- 𝑘  trees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.index.name='classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# display results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_As can be seen from these models, increasing training accuracy does not necessarily correspond with higher test accuracy. However, both the bagging and random forest models, which have high training accuracies, also have relatively high test accuracies. This is because they have low bias without increasing variance due to overfitting because they aggregate different trees which combine to negate each other's variance. This is compared to single decision trees, which reduce bias by increasing tree depth, but which can increase variance by overfitting on the training dataset, leading to challenges when fitting to test datasets, as can be seen in the single overfit tree with depth=20_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part4\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 4: Boosting [30 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.2",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.1** The following code (see code cell below) \"attempts\" to implement a simplified version of boosting using just two classifiers. However, this implementation has both fuctionality AND stylistic flaws. Imagine that you are a grader for a college course in Data Science. Write a set of grading comments (in the provided Markdown cell) for the student who submitted this code. Point out the flaws in their provided code submission.\n",
    "\n",
    "The intended functionality (i.e. expected requirements) of this \"attempted\" code is to accomplish the following:\n",
    "\n",
    "1. Fit an initial tree with a maximum depth of 3.\n",
    "2. Construct an array of sample weights that give a weight of 1 to samples that the initial tree classified correctly, and a weight of 2 to samples that the initial tree misclassified.\n",
    "3. Fit a second depth-3 decision tree using those sample weights.\n",
    "4. Predict by computing the probabilities that the initial tree and the second tree each assign to the positive class, then take the average of those two probabilities as the prediction probability.\n",
    "5. Report the training and test accuracies of just the initial tree, as well the training and test accuracies of the full 2-tree boosting ensemble.\n",
    "\n",
    "**NOTE:** Please do not modify anything in the code cell itself.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL\n",
    "# \"Attempted\" boosting implementation\n",
    "\n",
    "def boostmeup(X, y):\n",
    "    tree = DecisionTreeClassifier(max_depth=3)\n",
    "    tree1 = tree.fit(X, y)\n",
    "    sample_weight = np.ones(len(X_train))\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "          if tree1.predict([X_train[idx]]) != y_train[idx]:\n",
    "             sample_weight[idx] = sample_weight[idx] * 2\n",
    "             q = q + 1\n",
    "    print(\"tree1 accuracy:\", q / len(X_train))\n",
    "    tree2 = tree.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    \n",
    "# Train\n",
    "    q = 0\n",
    "    for idx in range(len(X_train)):\n",
    "        t1p = tree1.predict_proba([X_train[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_train[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_train))\n",
    "\n",
    "# Test\n",
    "    q = 0\n",
    "    for idx in range(len(X_test)):\n",
    "        t1p = tree1.predict_proba([X_test[idx]])[0][1]\n",
    "        t2p = tree2.predict_proba([X_test[idx]])[0][1]\n",
    "        m = (t1p + t2p) / 2\n",
    "        if m > .5:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 0\n",
    "            else:\n",
    "                q = q + 1\n",
    "        else:\n",
    "            if y_train[idx] == True:\n",
    "                q = q + 1\n",
    "            else:\n",
    "                q = 0\n",
    "    print(\"Boosted accuracy:\", q / len(X_test))\n",
    "\n",
    "boostmeup(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_1) Name variables meaningfully; 2)Function should take X_train, y_train, X_test, y_test as arguments & then not call global variables X_train and X_test; 3) Variable q should be named differently when used multiple times/don't repeat the use of q; 4)Could use list comprehension instead of longer for loop syntax; 5) Make a function for boosting and call that for train & test, rather than repeating code for bothl; 6) Indexing of predict_proba values could use the ' :, ' operators; 7) q should be increased by 1 if y_train[idx] == True, not otherwise_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.2** Now, imagine that you are the Teaching Fellow responsible for writing the \"solutions\" code for the simplified version of boosting using just two classifiers that had been \"attempted\" in Question 4.1:\n",
    "\n",
    "- Write an **excellent** example implementation from scratch (i.e. using just scikit-learn's `DecisionTreeClassifier` and NumPy to perform your boosting). Your implementation should be written either [functionally](https://docs.python.org/3/tutorial/controlflow.html#defining-functions) or as a [class](https://docs.python.org/3/tutorial/classes.html), such that you can then call the function(s) or class methods to generate your predictions and/or accuracy scores.\n",
    "\n",
    "- Report on the performance of your boosting algorithm by printing the training and test accuracies of just the initial tree, as well the training and test accuracies of the full 2-tree boosting ensemble.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "def boosting_pred(X, trees):\n",
    "    mean_pred_proba = sum(tree.predict_proba(X)[:, 1] for tree in trees) / len(trees)\n",
    "    binary_pred = (mean_pred_proba > 0.5).astype(int)\n",
    "    return binary_pred\n",
    "\n",
    "\n",
    "# actually doing the boosting here\n",
    "def boosting(X_train, y_train, X_test, y_test, num_iterations):\n",
    "    result_dict = {'Train Acc': [], 'Test Acc': []}\n",
    "    \n",
    "    # Build the initial tree classifier with depth of 3\n",
    "    tree1 = DecisionTreeClassifier(max_depth=3)\n",
    "    tree1.fit(X_train, y_train)\n",
    "    train_acc, test_acc = tree1.score(X_train, y_train), tree1.score(X_test, y_test)\n",
    "    result_dict['Train Acc'].append(train_acc)\n",
    "    result_dict['Test Acc'].append(test_acc)\n",
    "    \n",
    "    # save all the sequential models\n",
    "    trees = [tree1]\n",
    "    \n",
    "    # start our boosting iteration\n",
    "    for t in range(num_iterations):\n",
    "        prev_train_preds = boosting_pred(X_train, trees)\n",
    "        prev_test_preds = boosting_pred(X_test, trees)\n",
    "        \n",
    "        # determine the weight according to the previous predictions\n",
    "        sample_weight = np.where(prev_train_preds == y_train, 1.0, 2.0)\n",
    "        \n",
    "        # fit the new weak classifier\n",
    "        current_tree = DecisionTreeClassifier(max_depth=3)\n",
    "        current_tree.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        trees.append(current_tree)\n",
    "        \n",
    "        # make the predictions using the new ensemble\n",
    "        train_preds = boosting_pred(X_train, trees)\n",
    "        test_preds = boosting_pred(X_test, trees)\n",
    "        train_acc = (train_preds == y_train).sum() / len(y_train)\n",
    "        test_acc = (test_preds == y_test).sum() / len(y_test)\n",
    "        result_dict['Train Acc'].append(train_acc)\n",
    "        result_dict['Test Acc'].append(test_acc)\n",
    "    \n",
    "    result_df = pd.DataFrame(result_dict)\n",
    "    result_df.index = ['Iteration ' + str(i) for i in range(1, num_iterations + 2)]\n",
    "    return result_df\n",
    "    \n",
    "result_df = boosting(X_train, y_train, X_test, y_test, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.3** Now, let us use the scikit-learn implementation of AdaBoost. For the sake of simplicity we'll use a single validation split for hyperparameter tuning.\n",
    "    \n",
    "Use `AdaBoostClassifier` to fit another ensemble to the reduced training set, `X_train80`. Use a decision tree of depth-3 as the base learner, a learning rate 0.05, the default algorithm `SAMME.R`, and run the boosting for 800 iterations. Make a plot of the effect of the number of iterations on the model's train and validation accuracy.\n",
    "\n",
    "**HINT:** The `.staged_score(...)` method provides the accuracy numbers you'll need for plotting. You'll need to use `list(...)` to convert the \"generator\" that `staged_score` returns into an ordinary list.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train/validation split for hyperparameter tuning\n",
    "X_train80, X_val, y_train80, y_val = train_test_split(X_train,\n",
    "                                                      y_train,\n",
    "                                                      train_size=.8,\n",
    "                                                      random_state=109,\n",
    "                                                      stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "adaboost_model = AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth=3), n_estimators=800, learning_rate=0.05).fit(X_train80, y_train80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ada_train_acc=list(adaboost_model.staged_score(X_train80,y_train80))\n",
    "ada_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ada_val_acc=list(adaboost_model.staged_score(X_val,y_val))\n",
    "ada_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "# your code here\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plotswith labels\n",
    "ax.plot(list(range(1,801)),ada_train_acc, label='train accuracies')\n",
    "ax.plot(list(range(1,801)),ada_val_acc, label='val accuracies')\n",
    "\n",
    "# Set the plot labels and legends\n",
    "ax.set_xlabel('No. of Boosting Iterations')\n",
    "ax.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "4.4",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.4** Repeat the plot above for a base learner with depths of 1, 2, 3, and 4. For easier comparison you should arrange your plots in a row. What trends do you see in the training and validation accuracies and how would you explain this behavior?\n",
    "\n",
    "**NOTE:** It is okay if your code re-fits the depth-3 classifier instead of reusing the results from the previous problem.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_depths=[1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ada_train_accs=[]\n",
    "ada_val_accs=[]\n",
    "# your code here\n",
    "for depth in boosting_depths:\n",
    "    adaboost_model = AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth=depth), n_estimators=800, learning_rate=0.05).fit(X_train80, y_train80)\n",
    "    ada_train_accs.append(list(adaboost_model.staged_score(X_train80,y_train80)))\n",
    "    ada_val_accs.append(list(adaboost_model.staged_score(X_val,y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "fig, ax = plt.subplots(ncols=len(boosting_depths),nrows=1,figsize=(20,5))\n",
    "ax=ax.ravel() #flattening ax so that can be looped through, rather than all selected simultaneously\n",
    "for depth in boosting_depths:\n",
    "\n",
    "\n",
    "    # Plotswith labels\n",
    "    ax[depth-1].plot(list(range(1,801)),ada_train_accs[depth-1], label='train accuracies')\n",
    "    ax[depth-1].plot(list(range(1,801)),ada_val_accs[depth-1], label='val accuracies')\n",
    "\n",
    "    # Set the plot labels and legends\n",
    "    ax[depth-1].set_xlabel('No. of Boosting Iterations')\n",
    "    ax[depth-1].legend(loc = 'best')\n",
    "    ax[depth-1].set_title(\"Max Depth - \" + str(depth), fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.5** Based on the plots from Question 4.4, what combination of base learner depth and the number of iterations seems optimal and why?\n",
    "\n",
    "**Note:** Feel free to make use of additional code to make your decision here if that is helpful. \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "max_val_acc=0\n",
    "best_ada_depth=0\n",
    "best_ada_iter=0\n",
    "for depth in boosting_depths:\n",
    "    #print(max(ada_val_accs[depth-1]))\n",
    "    if max(ada_val_accs[depth-1])>max_val_acc:\n",
    "        max_val_acc=max(ada_val_accs[depth-1])\n",
    "        best_ada_depth=depth\n",
    "        best_ada_iter=ada_val_accs[depth-1].index(max(ada_val_accs[depth-1]))+1\n",
    "print(max_val_acc)\n",
    "print(best_ada_depth)\n",
    "print(best_ada_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The optimal base learner depth and no. of iterations seems to be 2 & 333, respectively. This is based on the best validation accuracy for the AdaBoost classifiers of different depths. Although this model overfits, when you compare the train and validation accuracies, given that the issue with overfitting is that it can cause a model to underperform on non-training datasets, as this model still provides the best validation accuracy, I've chosen it._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**4.6** Fit a final AdaBoostClassifier on the **entire train set** using the base learner depth and number of iterations you identified as optimal in the previous question. Keep the learning rate at 0.05 as before. Store the train and test accuracies in `ada_train_acc` and `ada_test_acc` respectively. How does the performance of this model compare with the performance of the ensemble methods you considered in Question 2 and Question 3?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_model4_6 = AdaBoostClassifier(base_estimator= DecisionTreeClassifier(max_depth=best_ada_depth), n_estimators=best_ada_iter, learning_rate=0.05).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "ada_train_acc = adaboost_model4_6.score(X_train,y_train)\n",
    "ada_test_acc = adaboost_model4_6.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Ada Boost Train Accuracy: {ada_train_acc:.2%}\")\n",
    "print(f\"Ada Boost Test Accuracy: {ada_test_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": []
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<a id=\"part5\"></a>\n",
    "\n",
    "## <div class='exercise'>Question 5: Understanding [15 pts]</div> \n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "This question is intended to evaluate your overall knowledge and understanding of the current material. You may need to refer to lecture notes and other course materials to answer these questions.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.1",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**5.1** How do boosting and bagging relate: what is common to both, and what is unique to each?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Both are methods of improving model performance by increasing the amount of variance in the data that can be explained by the model by aggregating different models. However, boosting does this combining many 'weak learners' to make a 'strong learner' by fitting successive trees of low depth, each tree after the first fit with the residuals of the first tree, rather than the originial outcome variable. This has the effect of fitting the model to the previous mistakes, making it stronger. By contrast bagging improves performance by aggregating different models, each of which has been fit with a random subset of the original feature set, to reduce correlation between the different models._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.3",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**5.2** What is the impact of having too many trees in boosting and in bagging? In which instance is it worse to have too many trees?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_In bagging, increasing the number of trees doesn't lead to overfitting, because the final model is an aggregate of different trees, each of which has a limited depth, preventing overfitting. In boosting, you can quickly overfit with additional trees, as each successive tree is fit to the residuals/mistakes of the previous tree. Therefore, it is worse to have too many trees in boosting._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.4",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**5.3** Which technique, boosting or bagging, is better suited to parallelization, where you could have multiple computers working on a problem at the same time? Why?\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Bagging is more suited to parallelization, because it is a parallel ensemble model, as different, independent models are fit, the results of which are then aggregated. Boosting is sequential, as each tree is fit to the residuals/mistakes of the previous tree._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "5.5",
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class='exercise-r'>\n",
    "\n",
    "**5.4** Which of these techniques can be extended to regression tasks? Describe how this can be done.\n",
    "\n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Both can be applied to regression. In bagging, the output of individual regression models can be averaged to get the output of the bagged ensemble model. In boosting, a naive prediction - the mean of the outcome - is initially made and then the model is fit to the residuals of this first model, instead of the original outcome variable. Successive models are fit to the residuals of the previous model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "jupyter": {
     "outputs_hidden": true
    },
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "**This concludes HW6. Thank you!**\n",
    "\n",
    "[Return to contents](#contents)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.1": {
     "name": "q1.1",
     "points": 7,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert len(train_scores) == 20, \"Your train_scores should be a list or array containing 20 values\"\n>>> assert len(cvmeans) == 20, \"Your cvmeans should be a list or array containing 20 values\"\n>>> assert len(cvstds) == 20, \"Your cvstds should be a list or array containing 20 values\"\n>>> assert np.isclose(np.mean(train_scores), 0.84211, atol=0.001), \"The values in your train_scores list/array are incorrect\"\n>>> assert np.isclose(np.mean(cvmeans), 0.6103, atol=0.001), \"The values in your cvmeans list/array are incorrect\"\n>>> assert np.isclose(np.mean(cvstds), 0.0103326, atol=0.001), \"The values in your cvstds list/array are incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert bagging_train_df.shape == (5000, 55), \"Check the dimensions of your bagging_train_df\"\n>>> assert bagging_test_df.shape == (5000, 55), \"Check the dimensions of your bagging_test_df\"\n>>> assert len(bagging_models_list) == 55, \"You should have 55 estimators in your bagging_models_list\"\n>>> assert bagging_train_df.sum().sum() == 144604, \"The values in your bagging_train_df are incorrect. Make sure you are using the right random seed.\"\n>>> assert bagging_test_df.sum().sum() == 145385, \"The values in your bagging_test_df are incorrect. Make sure you are using the right random seed.\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.3": {
     "name": "q2.3",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert bagging_accuracy_train > 0.9 and bagging_accuracy_train <= 1, \"Your bagging train accuracy is very off\"\n>>> assert bagging_accuracy_test > 0.60 and bagging_accuracy_test < 0.8, \"Your bagging test accuracy is very off\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": 3,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert random_forest_train_score > 0.9 and random_forest_train_score <= 1, \"Your RF train accuracy is very off\"\n>>> assert random_forest_test_score > 0.60 and random_forest_test_score < 0.8, \"Your RF test accuracy is very off\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.2": {
     "name": "q3.2",
     "points": 6,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert type(top_predictors_bagging) == type(pd.DataFrame()), \"top_predictors_bagging should be a DataFrame\"\n>>> assert type(top_predictors_rf) == type(pd.DataFrame()), \"top_predictors_rf should be a DataFrame\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
